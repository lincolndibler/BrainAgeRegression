{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNxdyGPN00QTNpMhkkiPKCy"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# 1.) Imports"],"metadata":{"id":"CTKJ9wpFHREJ"}},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import scipy.ndimage\n","import random\n","import torch.optim as optim\n","from torch.optim.lr_scheduler import ReduceLROnPlateau\n","import pandas as pd\n","\n","import torch\n","import random"],"metadata":{"id":"6at-yUF1D8AA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BrainAgeTrainer:\n","    def __init__(self, model, train_loader, val_loader, criterion, optimizer, device,\n","                 scheduler=None, augment=True, early_stopping_patience=3, use_weighted_loss=False):\n","        self.model = model\n","        self.train_loader = train_loader\n","        self.val_loader = val_loader\n","        self.criterion = criterion\n","        self.optimizer = optimizer\n","        self.scheduler = scheduler\n","        self.device = device\n","        self.augment = augment\n","        self.use_weighted_loss = use_weighted_loss\n","\n","        self.history = {'train_loss': [], 'val_loss': []}\n","        self.train_preds = []\n","        self.train_targets = []\n","        self.val_preds = []\n","        self.val_targets = []\n","        self.early_stopping_patience = early_stopping_patience\n","        self.best_val_loss = float('inf')\n","        self.epochs_without_improvement = 0\n","        self.best_model_state = None\n","\n","    def train(self, epochs=10, track_predictions=False):\n","        self.model.to(self.device)\n","\n","        for epoch in range(epochs):\n","            print(f\"\\nüîÅ Epoch {epoch+1}/{epochs}\")\n","            self.model.train()\n","            train_loss = 0.0\n","\n","            if track_predictions:\n","                self.train_preds.clear()\n","                self.train_targets.clear()\n","\n","            for i, batch in enumerate(self.train_loader):\n","                if self.use_weighted_loss:\n","                    images, ages, weights = batch\n","                    weights = weights.to(self.device)\n","                else:\n","                    images, ages = batch\n","                    weights = None\n","\n","                images = images.to(self.device)\n","                ages = ages.to(self.device).view(-1)\n","\n","                if self.augment:\n","                    images = torch.stack([self.augment_volume(img) for img in images])\n","\n","                self.optimizer.zero_grad()\n","                outputs = self.model(images)\n","\n","                if self.use_weighted_loss:\n","                    loss = torch.mean(weights * torch.abs(outputs - ages))\n","                else:\n","                    loss = self.criterion(outputs, ages)\n","\n","                if hasattr(self.model, 'age_scale') and hasattr(self.model, 'age_bias'):\n","                    reg_lambda = 0.01\n","                    reg_term = reg_lambda * (self.model.age_scale**2 + self.model.age_bias**2)\n","                    loss = loss + reg_term\n","\n","                loss.backward()\n","                self.optimizer.step()\n","\n","                train_loss += loss.item() * images.size(0)\n","\n","                if track_predictions:\n","                    self.train_preds.extend(outputs.detach().cpu().numpy().reshape(-1))\n","                    self.train_targets.extend(ages.cpu().numpy().reshape(-1))\n","\n","            train_loss /= len(self.train_loader.dataset)\n","            val_loss = self._validate(track_predictions)\n","\n","            self.history['train_loss'].append(train_loss)\n","            self.history['val_loss'].append(val_loss)\n","\n","            print(f\"üìâ Train Loss: {train_loss:.4f} | üß™ Val Loss: {val_loss:.4f}\")\n","\n","            if hasattr(self.model, 'age_scale') and hasattr(self.model, 'age_bias'):\n","                scale = self.model.age_scale.item()\n","                bias = self.model.age_bias.item()\n","                print(f\"üßÆ Calibration ‚Üí age_scale: {scale:.4f}, age_bias: {bias:.4f}\")\n","\n","            if val_loss < self.best_val_loss:\n","                self.best_val_loss = val_loss\n","                self.epochs_without_improvement = 0\n","                self.best_model_state = self.model.state_dict()\n","                print(\"üíæ Best model updated\")\n","            else:\n","                self.epochs_without_improvement += 1\n","                if self.epochs_without_improvement >= self.early_stopping_patience:\n","                    print(\"üõë Early stopping triggered\")\n","                    break\n","\n","            if self.scheduler:\n","                self.scheduler.step(val_loss)\n","                current_lr = self.optimizer.param_groups[0]['lr']\n","                print(f\"üìâ Current Learning Rate: {current_lr:.2e}\")\n","\n","        if self.best_model_state is not None:\n","            self.model.load_state_dict(self.best_model_state)\n","            print(\"‚úÖ Restored best model weights\")\n","\n","    def _validate(self, track_predictions):\n","        self.model.eval()\n","        val_loss = 0.0\n","\n","        if track_predictions:\n","            self.val_preds.clear()\n","            self.val_targets.clear()\n","\n","        with torch.no_grad():\n","            for images, ages in self.val_loader:\n","                images, ages = images.to(self.device), ages.to(self.device)\n","                outputs = self.model(images)\n","                loss = self.criterion(outputs, ages)\n","                val_loss += loss.item() * images.size(0)\n","\n","                if track_predictions:\n","                    self.val_preds.extend(outputs.cpu().numpy().reshape(-1))\n","                    self.val_targets.extend(ages.cpu().numpy().reshape(-1))\n","\n","        return val_loss / len(self.val_loader.dataset)\n","\n","    def get_history(self):\n","        return self.history\n","\n","    def get_predictions(self):\n","        return {\n","            'train': (self.train_preds, self.train_targets),\n","            'val': (self.val_preds, self.val_targets)\n","        }\n","\n","    def augment_volume(self, volume, flip_prob=0.5, noise_std=0.002):\n","        volume = volume.clone()\n","\n","        if random.random() < flip_prob:\n","            volume = torch.flip(volume, dims=[1])\n","        if random.random() < flip_prob:\n","            volume = torch.flip(volume, dims=[2])\n","        if random.random() < flip_prob:\n","            volume = torch.flip(volume, dims=[3])\n","\n","        volume += torch.randn_like(volume) * noise_std\n","\n","        scale = random.uniform(0.95, 1.05)\n","        shift = random.uniform(-0.05, 0.05)\n","        volume = volume * scale + shift\n","\n","        return volume.to(self.device)\n","\n","def compute_age_weights(df, bins=8, normalize=True):\n","    df = df.copy()\n","    df['age_bin'] = pd.cut(df['Age'], bins=bins)\n","    bin_counts = df['age_bin'].value_counts().sort_index()\n","    weights = 1.0 / bin_counts\n","\n","    if normalize:\n","        weights /= weights.max()\n","\n","    return dict(zip(bin_counts.index, weights))\n","\n","def compute_balanced_age_weights(df, bins=8, normalize=True, custom_boost=None):\n","    df = df.copy()\n","    df['age_bin'] = pd.qcut(df['Age'], q=bins, duplicates='drop')\n","    bin_counts = df['age_bin'].value_counts().sort_index()\n","    weights = 1.0 / bin_counts.astype(float)\n","\n","    if custom_boost is not None:\n","        assert len(custom_boost) == len(weights), f\"custom_boost must match number of bins ({len(weights)})\"\n","        weights *= np.array(custom_boost)\n","\n","    if normalize:\n","        weights /= weights.max()\n","\n","    print(\"\\nüìä Age Bin Weights:\")\n","    for bin_interval, weight in zip(bin_counts.index, weights):\n","        print(f\"{bin_interval}: {weight:.4f}\")\n","\n","    return dict(zip(bin_counts.index, weights))"],"metadata":{"id":"HvFcG8L0D693"},"execution_count":null,"outputs":[]}]}